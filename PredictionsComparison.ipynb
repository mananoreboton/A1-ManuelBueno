{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f663d3-74aa-48bf-b79c-24f3e4a6ff5e",
   "metadata": {},
   "source": [
    "## Select and analysis dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a47ea4-2c63-4128-b5fd-8e3b184dd56e",
   "metadata": {},
   "source": [
    "First, we call PreprocessData.select_and_analyze_dataset() to prepare the input dataset and save the train and test data to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0eaa87-096f-4a30-b641-a9b98111760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreprocessData initialized.\n",
      "Reading data from ./data/kc_house_data.csv...\n",
      "Truncating data randomly to 2000 rows\n",
      "Selecting this columns from the data: ['date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'yr_built', 'lat', 'long', 'price']\n",
      "Removing missing values from columns: ['date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'yr_built', 'lat', 'long', 'price']\n",
      "Removing outliers values from columns: []\n",
      "Splitting data: train_data (1600) and test_data (400)\n",
      "Creating ColumnTransformer\n",
      "Fit train_data\n",
      "Transforming train_data and test_data\n",
      "Train matrix saved to ./data/transformed_train_matrix.csv\n",
      "Test matrix saved to ./data/transformed_test_matrix.csv\n",
      "Transformer saved to ./data/transformer.pkl\n",
      "Executed all subtasks of select and analyze dataset...\n"
     ]
    }
   ],
   "source": [
    "from PreprocessData import PreprocessData\n",
    "preprocessData = PreprocessData()\n",
    "preprocessData.select_and_analyze_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de66ef60-367a-4c39-92d7-8ec1f31a5b52",
   "metadata": {},
   "source": [
    "## Hyperparameter comparison and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf4cad-48d8-49be-bf9b-ffc0b941adcf",
   "metadata": {},
   "source": [
    "We will explore some of the space of hyperparameters, trying different combinations and \n",
    "evaluating the quality of the result of the prediction obtained using them.\n",
    "\n",
    "For that, we load the hyperparameter combinations and the transformed train dataset from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a94480-6bd7-457f-a9ed-02dc1bfc6644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Number of Layers        Layer Structure  Num Epochs  Learning Rate  \\\n",
      "0                  3            [22, 64, 1]         100         0.0010   \n",
      "1                  4       [22, 128, 64, 1]         150         0.0005   \n",
      "2                  2                [22, 1]          80         0.0020   \n",
      "3                  4       [22, 128, 64, 1]         120         0.0010   \n",
      "4                  5  [22, 256, 128, 64, 1]         200         0.0001   \n",
      "5                  4       [22, 128, 64, 1]         150         0.0005   \n",
      "6                  3            [22, 64, 1]         100         0.0010   \n",
      "7                  5  [22, 256, 128, 64, 1]         200         0.0002   \n",
      "8                  3           [22, 128, 1]         140         0.0005   \n",
      "9                  5  [22, 256, 128, 64, 1]         250         0.0001   \n",
      "10                 3            [22, 64, 1]         180         0.0005   \n",
      "\n",
      "    Momentum Activation Function  \n",
      "0       0.90                relu  \n",
      "1       0.95                tanh  \n",
      "2       0.85             sigmoid  \n",
      "3       0.90                relu  \n",
      "4       0.95              linear  \n",
      "5       0.90                relu  \n",
      "6       0.85                tanh  \n",
      "7       0.95             sigmoid  \n",
      "8       0.90                relu  \n",
      "9       0.95             sigmoid  \n",
      "10      0.85                relu  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "hyperparameters = pd.read_csv(\"data/neural_network_parameters.csv\")\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96718adb-05c5-4f52-90d4-ffdd2e58dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30319149 0.375      0.21875    0.10521739 0.00652308 1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.5\n",
      "  0.4        0.57391304 0.65979557 0.2078922 ]]\n",
      "[0.04393443]\n"
     ]
    }
   ],
   "source": [
    "X_in, y_in = preprocessData.read_transformed_data_from_file()\n",
    "print(X_in[:1])\n",
    "print(y_in[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9edca-c4c4-4d96-a128-97f983757f72",
   "metadata": {},
   "source": [
    "For each iteration over the combinations: \n",
    "- we create a new instance of the NeuralNet with the hyperparameters,\n",
    "- call the NeuralNet.fit() function with Y_in (instances) and y_in (ground truth target values) to train our neuronal network,\n",
    "- call the NeuralNet.predict() function to obtain the estimated target values (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f1a91-e12a-4a2f-bdc4-125eff03a4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet initialized with self.L = '3', self.n = '[22, 64, 1]', self.n_epochs = '100', self.learning_rate = '0.001', self.momentum = '0.9', self.fact = 'relu', self.validation_split = '0.2'\n",
      "Executing fit(X, y)\n",
      "NeuralNet initialized with self.L = '4', self.n = '[22, 128, 64, 1]', self.n_epochs = '150', self.learning_rate = '0.0005', self.momentum = '0.95', self.fact = 'tanh', self.validation_split = '0.2'\n",
      "Executing fit(X, y)\n",
      "NeuralNet initialized with self.L = '2', self.n = '[22, 1]', self.n_epochs = '80', self.learning_rate = '0.002', self.momentum = '0.85', self.fact = 'sigmoid', self.validation_split = '0.2'\n",
      "Executing fit(X, y)\n",
      "NeuralNet initialized with self.L = '4', self.n = '[22, 128, 64, 1]', self.n_epochs = '120', self.learning_rate = '0.001', self.momentum = '0.9', self.fact = 'relu', self.validation_split = '0.2'\n",
      "Executing fit(X, y)\n",
      "NeuralNet initialized with self.L = '5', self.n = '[22, 256, 128, 64, 1]', self.n_epochs = '200', self.learning_rate = '0.0001', self.momentum = '0.95', self.fact = 'linear', self.validation_split = '0.2'\n",
      "Executing fit(X, y)\n",
      "NeuralNet initialized with self.L = '4', self.n = '[22, 128, 64, 1]', self.n_epochs = '150', self.learning_rate = '0.0005', self.momentum = '0.9', self.fact = 'relu', self.validation_split = '0.2'\n",
      "Executing fit(X, y)\n",
      "NeuralNet initialized with self.L = '3', self.n = '[22, 64, 1]', self.n_epochs = '100', self.learning_rate = '0.001', self.momentum = '0.85', self.fact = 'tanh', self.validation_split = '0.2'\n",
      "Executing fit(X, y)\n",
      "NeuralNet initialized with self.L = '5', self.n = '[22, 256, 128, 64, 1]', self.n_epochs = '200', self.learning_rate = '0.0002', self.momentum = '0.95', self.fact = 'sigmoid', self.validation_split = '0.2'\n",
      "Executing fit(X, y)\n",
      "NeuralNet initialized with self.L = '3', self.n = '[22, 128, 1]', self.n_epochs = '140', self.learning_rate = '0.0005', self.momentum = '0.9', self.fact = 'relu', self.validation_split = '0.2'\n",
      "Executing fit(X, y)\n",
      "NeuralNet initialized with self.L = '5', self.n = '[22, 256, 128, 64, 1]', self.n_epochs = '250', self.learning_rate = '0.0001', self.momentum = '0.95', self.fact = 'sigmoid', self.validation_split = '0.2'\n",
      "Executing fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from NeuralNet import NeuralNet\n",
    "for _, params in hyperparameters.iterrows():\n",
    "    neural_net = NeuralNet(\n",
    "        L = params[\"Number of Layers\"],\n",
    "        n = eval(params[\"Layer Structure\"]),  # Convert string to list\n",
    "        n_epochs = params[\"Num Epochs\"],\n",
    "        learning_rate = params[\"Learning Rate\"],\n",
    "        momentum = params[\"Momentum\"],\n",
    "        activation_function = params[\"Activation Function\"],\n",
    "        validation_split = 0.2\n",
    "    )\n",
    "\n",
    "    neural_net.fit(X_in, y_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576804b0-f4e1-4e7f-b69c-bcfb35dc3367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
